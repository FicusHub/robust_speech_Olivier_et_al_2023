# General information
seed: 1001
__set_seed: !apply:torch.manual_seed [!ref <seed>]
root: !PLACEHOLDER
tokenizers_folder: !ref <root>/tokenizers

# Hyparameters below are dependant on the attack and model used 
# and should be changed at the user's discretion
# -------------------------------------------------------------
# Attack information
snr: 30
nb_epochs: 10
nb_iter: 20
lr: 0.001
eps: 0.01
eps_item: 0.003
time_universal: False

delta: !new:robust_speech.adversarial.utils.TensorModule
   size: (1,288000)

attack_class: !name:robust_speech.adversarial.attacks.universal.UniversalAttack
  targeted: False
  snr: !ref <snr>
  nb_epochs: !ref <nb_epochs>
  nb_iter: !ref <nb_iter>
  lr: !ref <lr>
  eps: !ref <eps>
  eps_item: !ref <eps_item>
  time_universal: !ref <time_universal>
  univ_perturb: !ref <delta>
save_audio: True

# Model information
model_name: asr-crdnn-transformerlm-librispeech-ctc
attack_name: universal
target_brain_class: !name:robust_speech.models.seq2seq.S2SASR
target_brain_hparams_file: !ref model_configs/<model_name>.yaml
# source_brain_class: null
# source_brain_hparams_file: null

# Tokenizer information (compatible with target and source)
pretrained_tokenizer_path: !ref speechbrain/asr-crdnn-transformerlm-librispeech
tokenizer: !new:sentencepiece.SentencePieceProcessor
# -------------------------------------------------------------

# Output files
output_folder: !ref <root>/attacks/<attack_name>/<model_name>/universal_<eps>
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>
log: !ref <output_folder>/log.txt
save_audio_path: !ref <output_folder>/save
params_subfolder: !PLACEHOLDER # replace with the checkpoint of your universal perturbation
params_folder: !ref <output_folder>/<params_subfolder>

# Pretrainer loading parameters
# Comment delta for attacker training, and move 
# Uncomment delta for evaluation
pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
   collect_in: !ref <output_folder>
   loadables:
      tokenizer: !ref <tokenizer>
      delta: !ref <delta>
   paths:
      tokenizer: !ref <pretrained_tokenizer_path>/tokenizer.ckpt
      delta: !ref <params_folder>/delta.ckpt
      

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
   checkpoints_dir: !ref <output_folder>
   recoverables:
      delta: !ref <delta>

dataset_prepare_fct: !name:robust_speech.data.librispeech.prepare_librispeech
dataio_prepare_fct: !name:robust_speech.data.librispeech.dataio_prepare

# Data files
data_folder: !ref <root>/data/LibriSpeech # e.g, /localscratch/LibriSpeech
csv_folder: !ref <data_folder>/csv # e.g, /localscratch/LibriSpeech
# If RIRS_NOISES dir exists in /localscratch/xxx_corpus/RIRS_NOISES
# then data_folder_rirs should be /localscratch/xxx_corpus
# otherwise the dataset will automatically be downloaded
test_splits: ["test-clean"]
skip_prep: True
ckpt_interval_minutes: 15 # save checkpoint every N min
data_csv_name: test-clean-20
test_csv:
   - !ref <data_folder>/csv/<data_csv_name>.csv
data_csv_name_train: dev-clean-20
train_csv: !ref <data_folder>/csv/<data_csv_name_train>.csv
batch_size: 1 # This works for 2x GPUs with 32GB
avoid_if_longer_than: 24.0
sorting: random

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

# Decoding parameters (only for text_pipeline)
blank_index: 0
bos_index: 1
eos_index: 2

test_dataloader_opts:
    batch_size: 1
train_dataloader_opts:
    batch_size: 1

logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <log>
