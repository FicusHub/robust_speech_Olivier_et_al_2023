# ############################################################################
# Model: E2E ASR with Transformer
# Encoder: Conformer Encoder
# Decoder: Transformer Decoder + (CTC/ATT joint) beamsearch + TransformerLM
# Tokens: unigram
# losses: CTC + KLdiv (Label Smoothing loss)
# Training: Librispeech 960h
# Authors:  Jianyuan Zhong, Titouan Parcollet, Samuele Cornell
# ############################################################################
# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1001
__set_seed: !apply:torch.manual_seed [!ref <seed>]
root: !PLACEHOLDER
output_folder: !ref <root>/attacks/none/<seed>
model_folder: !ref <root>/models
wer_file: !ref <output_folder>/wer.txt
save_folder: !ref <output_folder>
log: !ref <output_folder>/log.txt

# Language model (LM) pretraining
# NB: To avoid mismatch, the speech recognizer must be trained with the same
# tokenizer used for LM training. Here, we download everything from the
# speechbrain HuggingFace repository. However, a local path pointing to a
# directory containing the lm.ckpt and tokenizer.ckpt may also be specified
# instead. E.g if you want to use your own LM / tokenizer.

#attack_class: !name:robust_speech.adversarial.attacks.pgd.SNRPGDAttack
#  targeted: False
#  snr: 30
#  nb_iter: 15
attack_class: null
save_audio_path: null
#source_brain_class: !name:robust_speech.models.wav2vec2_pretrain.W2VPretrain
#source_brain_hparams_file: attack_configs/wav2vec2_contrastive/w2v2.yaml

target_brain_class: !name:robust_speech.models.seq2seq.S2SASR
target_brain_hparams_file: model_configs/asr-crdnn-rnnlm-librispeech.yaml

dataset_prepare_fct: !name:robust_speech.data.librispeech.prepare_librispeech
dataio_prepare_fct: !name:robust_speech.data.librispeech.dataio_prepare

pretrained_lm_tokenizer_path: speechbrain/asr-crdnn-rnnlm-librispeech

# Data files
data_folder: !ref <root>/data/LibriSpeech # e.g, /localscratch/LibriSpeech
# If RIRS_NOISES dir exists in /localscratch/xxx_corpus/RIRS_NOISES
# then data_folder_rirs should be /localscratch/xxx_corpus
# otherwise the dataset will automatically be downloaded
test_splits: ["test-clean-short"]
skip_prep: True
ckpt_interval_minutes: 15 # save checkpoint every N min
test_csv:
   - !ref <data_folder>/csv/sanity.csv
# Training parameters
# To make Transformers converge, the global bath size should be large enough.
# The global batch size is computed as batch_size * n_gpus * gradient_accumulation.
# Empirically, we found that this value should be >= 128.
# Please, set your parameters accordingly.
batch_size: 1 # This works for 2x GPUs with 32GB
sorting: random

# Feature parameters
sample_rate: 16000
n_fft: 400
n_mels: 80

# Decoding parameters (only for text_pipeline)
blank_index: 0
bos_index: 1
eos_index: 2

valid_dataloader_opts:
    batch_size: 1

test_dataloader_opts:
    batch_size: 1

pretrained_tokenizer_path: speechbrain/asr-crdnn-rnnlm-librispeech
tokenizer: !new:sentencepiece.SentencePieceProcessor

logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <log>

error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
acc_computer: !name:speechbrain.utils.Accuracy.AccuracyStats

# The pretrainer allows a mapping between pretrained files and instances that
# are declared in the yaml. E.g here, we will download the file lm.ckpt
# and it will be loaded into "lm" which is pointing to the <lm_model> defined
# before.

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
   collect_in: !ref <save_folder>
   loadables:
      tokenizer: !ref <tokenizer>
   paths:
      tokenizer: !ref <pretrained_tokenizer_path>/tokenizer.ckpt
